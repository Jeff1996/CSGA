{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 2])\n",
      "tensor([[[[9., 4., 7., 7., 7., 5., 8., 8.],\n",
      "          [4., 1., 4., 4., 2., 6., 7., 4.],\n",
      "          [2., 8., 0., 2., 9., 9., 1., 7.],\n",
      "          [7., 7., 4., 8., 2., 0., 2., 6.],\n",
      "          [6., 4., 0., 9., 8., 1., 7., 8.],\n",
      "          [4., 8., 3., 1., 0., 4., 9., 2.],\n",
      "          [7., 9., 5., 6., 5., 8., 3., 5.],\n",
      "          [5., 9., 1., 2., 4., 7., 7., 7.]]],\n",
      "\n",
      "\n",
      "        [[[1., 7., 0., 2., 2., 9., 3., 7.],\n",
      "          [1., 5., 1., 4., 1., 6., 8., 0.],\n",
      "          [5., 9., 2., 0., 6., 1., 0., 1.],\n",
      "          [8., 2., 6., 5., 2., 1., 3., 7.],\n",
      "          [3., 3., 2., 5., 3., 3., 6., 7.],\n",
      "          [1., 6., 6., 5., 3., 4., 0., 2.],\n",
      "          [4., 8., 0., 6., 6., 0., 9., 2.],\n",
      "          [3., 7., 4., 1., 5., 3., 0., 7.]]]])\n",
      "tensor([[[[9., 7.],\n",
      "          [6., 8.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2.],\n",
      "          [3., 3.]]]])\n"
     ]
    }
   ],
   "source": [
    "# tensor格式的分割标签下采样测试\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "img_size = 8\n",
    "target_size = 2\n",
    "\n",
    "label = torch.randint(0, 10, (2, 1, img_size, img_size), dtype=torch.float32)  # 示例标签\n",
    "# label_subsample = F.interpolate(label, scale_factor=target_size/img_size, mode='nearest')\n",
    "label_subsample = F.interpolate(label, (target_size, target_size), mode='nearest')\n",
    "\n",
    "\n",
    "print(label_subsample.shape)\n",
    "print(label)\n",
    "print(label_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0961, 0.6014, 0.2064, 0.0961],\n",
      "        [0.1942, 0.1942, 0.4174, 0.1942],\n",
      "        [0.1877, 0.1877, 0.2986, 0.3260],\n",
      "        [0.4954, 0.1682, 0.1682, 0.1682]])\n",
      "tensor([[0.0895, 0.1266, 0.2678, 0.5161],\n",
      "        [0.6959, 0.1537, 0.0752, 0.0752],\n",
      "        [0.2301, 0.2301, 0.2301, 0.3096],\n",
      "        [0.1692, 0.1692, 0.1837, 0.4779]])\n",
      "tensor(0.4051)\n"
     ]
    }
   ],
   "source": [
    "# affinity交叉熵损失计算测试\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "target_size = 4\n",
    "\n",
    "func = nn.ReLU()\n",
    "\n",
    "affinity = torch.randn((target_size, target_size), dtype=torch.float32)\n",
    "affinity = func(affinity)\n",
    "affinity = F.softmax(affinity, dim=-1)\n",
    "label = torch.randn((target_size, target_size), dtype=torch.float32)\n",
    "label = func(label)\n",
    "label = F.softmax(label, dim=-1)\n",
    "\n",
    "loss = (-label*affinity.log()).mean()\n",
    "print(affinity)\n",
    "print(label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1065, 0.7870, 0.1065]])\n",
      "tensor([[-2.2395, -0.2395, -2.2395]])\n",
      "tensor([[0., 1., 0.]])\n",
      "KLDivLoss:  0.2395448386669159\n",
      "CrossEntropyLoss:  0.2395448386669159\n"
     ]
    }
   ],
   "source": [
    "# KL散度损失与交叉熵损失对比\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设我们有一个batch_size为2，类别数为3的模型输出（probabilities）\n",
    "# 和对应的软标签（probabilities）\n",
    "batch_size = 1\n",
    "num_classes = 3\n",
    "\n",
    "# 随机生成模型输出（probabilities），确保每行的和为1（或接近1）\n",
    "# model_outputs = torch.rand(batch_size, num_classes)\n",
    "model_outputs = torch.tensor([[1, 3, 1]], dtype=torch.float32)\n",
    "# model_outputs = F.softmax(model_outputs, dim=1)\n",
    "print(model_outputs.softmax(dim=-1))\n",
    "# 由于KLDivLoss期望输入是log-probabilities，我们需要对模型输出应用log变换\n",
    "# log_model_outputs = F.log_softmax(model_outputs, dim=1)\n",
    "log_model_outputs = F.log_softmax(model_outputs, dim=1)\n",
    "print(log_model_outputs)\n",
    "\n",
    "# 随机生成软标签（probabilities），确保每行的和为1（或接近1）\n",
    "# soft_labels = torch.rand(batch_size, num_classes)\n",
    "# soft_labels = F.softmax(soft_labels, dim=1)  # 确保软标签是有效的概率分布\n",
    "\n",
    "# soft_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "# soft_labels = F.one_hot(soft_labels, num_classes).float()\n",
    "soft_labels = torch.tensor([[0.0, 1.0, 0.0]], dtype=torch.float32)\n",
    "print(soft_labels)\n",
    "\n",
    "criterion = nn.KLDivLoss(reduction='batchmean', log_target=False)  # 注意：这里log_target应该设置为False，因为我们传递的是probabilities，\n",
    "loss = criterion(log_model_outputs, soft_labels)\n",
    "# 打印损失\n",
    "print('KLDivLoss: ', loss.item())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(log_model_outputs, soft_labels)\n",
    "print('CrossEntropyLoss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss:  -1.390041901028205e-11\n",
      "KLDivLoss:  -6.832333951933833e-07\n"
     ]
    }
   ],
   "source": [
    "# KL散度损失\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设我们有一个batch_size为2，类别数为3的模型输出（probabilities）\n",
    "# 和对应的软标签（probabilities）\n",
    "batch_size = 4\n",
    "num_heads = 3\n",
    "L = 128**2\n",
    "num_classes = 256\n",
    "\n",
    "# 模型输出\n",
    "model_outputs = F.softmax(torch.randn((batch_size, num_heads, L, num_classes), dtype=torch.float32) * 10, dim=-1)\n",
    "log_model_outputs = model_outputs.log()\n",
    "\n",
    "# print(model_outputs)\n",
    "\n",
    "# 随机生成软标签（probabilities），确保每行的和为1（或接近1）\n",
    "# soft_labels = torch.randint(0, num_classes, (batch_size, num_heads, L))\n",
    "# soft_labels = model_outputs.argmax(dim=-1)\n",
    "# soft_labels_onehot = F.one_hot(soft_labels, num_classes).float()\n",
    "\n",
    "soft_labels_onehot = model_outputs\n",
    "\n",
    "# print(soft_labels)\n",
    "# print(soft_labels_onehot)\n",
    "\n",
    "# 计算KL散度损失\n",
    "criterion = nn.KLDivLoss(reduction='batchmean', log_target=False)  # 注意：这里log_target应该设置为False，因为我们传递的是probabilities，\n",
    "loss = criterion(log_model_outputs.flatten(0,-2), soft_labels_onehot.flatten(0,-2))\n",
    "print('KLDivLoss: ', loss.item())\n",
    "loss = criterion(log_model_outputs, soft_labels_onehot)\n",
    "print('KLDivLoss: ', loss.item())\n",
    "\n",
    "# # 计算交叉熵损失\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# loss = criterion(model_outputs.flatten(0,-2), soft_labels_onehot.flatten(0,-2))\n",
    "# print('CrossEntropyLoss: ', loss.item())\n",
    "# loss = criterion(model_outputs.flatten(0,-2), soft_labels.flatten())\n",
    "# print('CrossEntropyLoss: ', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6214, -1.0354, -2.2281])\n",
      "tensor([-0.6214, -1.0354, -2.2281])\n"
     ]
    }
   ],
   "source": [
    "# softmax与log_softmax对比\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.randn(3)\n",
    "print(F.log_softmax(a, dim=-1))\n",
    "print(F.softmax(a, dim=-1).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16384, 256])\n"
     ]
    }
   ],
   "source": [
    "# 统计二维及以上维度矩阵某一行元素中各元素出现的频率\n",
    "import torch\n",
    "\n",
    "def getBlocks(x_2d: torch.Tensor, stride: int = 4):\n",
    "    '''\n",
    "    x_2d    : [batch_size, dim, height, width]\n",
    "    stride  : 滑窗尺寸\n",
    "    '''\n",
    "    B, dim, H, W = x_2d.shape\n",
    "    bs = stride ** 2                                                # block size\n",
    "    nb = int(H*W / bs)                                              # num blocks\n",
    "    x_unfold = F.unfold(                                            # [B, nb, bs, dim]\n",
    "        x_2d.float(), \n",
    "        kernel_size=stride, \n",
    "        padding=0, \n",
    "        stride=stride\n",
    "    ).view(B, dim, bs, nb).permute(0, 3, 2, 1).contiguous()         # [B, dim*bs, nb] -> [B, dim, bs, nb] -> [B, nb, bs, dim]\n",
    "    return x_unfold\n",
    "\n",
    "num_classes = 150\n",
    "B = 8\n",
    "dim = 1\n",
    "H = 128\n",
    "W = 128\n",
    "stride = 8\n",
    "L_ = H*W // stride**2\n",
    "\n",
    "fm = torch.randint(0, num_classes, (B, dim, H, W))              # [B, dim, H, W]\n",
    "# print(fm)\n",
    "\n",
    "# input = torch.tensor([0, 1, 0, 1, 2, 2, 3, 3, 3, 149])\n",
    "# input = torch.randint(0, num_classes, (B, L_, d))\n",
    "input = getBlocks(fm, stride).squeeze().long()                  # (B, L_, bs)\n",
    "# print(input)\n",
    "\n",
    "index_add = torch.arange(0, B*L_).reshape(B, L_, 1) * num_classes\n",
    "# print(input.shape)\n",
    "# print(index_add.shape)\n",
    "\n",
    "input_encode = input + index_add\n",
    "input_encode_flatten = input_encode.flatten()\n",
    "# print(input_encode)\n",
    "\n",
    "counts = torch.bincount(input_encode_flatten)\n",
    "padding = B * L_ * num_classes - counts.shape[0]\n",
    "counts = torch.cat([counts, torch.zeros(padding, dtype=torch.long)], dim=0)    \n",
    "# print(counts)\n",
    "\n",
    "frequency = counts.reshape(B, L_, num_classes) / stride**2  # [B, L_, num_classes]\n",
    "# print(frequency)\n",
    "\n",
    "# [B, L, L_]\n",
    "fm = fm.flatten(1)\n",
    "results = []\n",
    "for i in range(B):\n",
    "    results.append(frequency[i][:, fm[i]].transpose(-2, -1))\n",
    "results = torch.stack(results)\n",
    "print(results.shape)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affinity loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# affinity loss测试\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def getMask(size: tuple, index_onehot: torch.Tensor, gain: float=1.0):\n",
    "    '''\n",
    "    size: 特征图尺寸, (h, w)\n",
    "    index_onehot: 聚类结果(每个像素对应的聚类中心的one-hot索引), [B, num_heads, L, S]\n",
    "    gain: 增益系数\n",
    "    '''\n",
    "    assert type(size) == tuple, 'Data type of size in function <getMask> should be <tuple>!'\n",
    "    assert size.__len__() == 2, 'Length of size should be 2!'\n",
    "    coords_h = torch.arange(size[0])\n",
    "    coords_w = torch.arange(size[1])\n",
    "    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 构造坐标窗口元素坐标索引，[2, h, w]\n",
    "    # 一维化特征图像素坐标，[2, L]\n",
    "    coords_featuremap = torch.flatten(coords, start_dim=1).float().to(index_onehot.device)\n",
    "    # [B, num_heads, 2, L]\n",
    "    coords_featuremap = coords_featuremap.reshape(\n",
    "        1, 1, 2, -1\n",
    "    ).repeat(index_onehot.shape[0], index_onehot.shape[1], 1, 1)\n",
    "    # [B, num_heads, 1, S]\n",
    "    index_onehot_sum = torch.sum(index_onehot, dim=-2, keepdim=True)\n",
    "    index_onehot_sum[index_onehot_sum==0] = 1\n",
    "    \n",
    "    # 聚类中心坐标，[B, num_heads, 2, S]\n",
    "    coords_clustercenter = torch.einsum(\n",
    "        'bhcl,bhls->bhcs', coords_featuremap, index_onehot\n",
    "    ) / index_onehot_sum\n",
    "\n",
    "    # 构造相对位置矩阵, 第一个矩阵是h方向的相对位置差, 第二个矩阵是w方向的相对位置差\n",
    "    relative_coords = coords_featuremap[:, :, :, :, None] - coords_clustercenter[:, :, :, None, :]\n",
    "    distance = torch.sqrt(                                      # [B, num_heads, L, S]\n",
    "        torch.square(relative_coords[:,:,0,:,:]) + torch.square(relative_coords[:,:,1,:,:])\n",
    "    )\n",
    "    # exp操作用于处理distance中的0, [B, num_heads, L, S]\n",
    "    distance_exp = torch.exp(distance)\n",
    "    # 距离越远的token注意力增强越少(加性增强), 最大值为1*gain, 最小值可以接近0, [B, num_heads, L, S]\n",
    "    mask = (1 / distance_exp) * gain\n",
    "    return mask\n",
    "\n",
    "def initRPE(shape_x, shape_c, window_size):\n",
    "    '''\n",
    "    输出：\n",
    "    delta_onehot\n",
    "    relative_position_bias_init\n",
    "    '''\n",
    "    batch_size, num_heads, L_, _ = shape_c\n",
    "    _, _, H, W, _ = shape_x\n",
    "    H_, W_ = H // window_size[0], W // window_size[1]\n",
    "    N = window_size[0] * window_size[1]\n",
    "\n",
    "    delta_index = torch.arange(L_).reshape(1, 1, L_, 1).repeat(batch_size, num_heads, 1, N)\n",
    "    delta_index = delta_index.reshape(\n",
    "        batch_size, num_heads, H_, W_, window_size[0], window_size[1]\n",
    "    ).permute(\n",
    "        0, 1, 2, 4, 3, 5\n",
    "    ).reshape(\n",
    "        batch_size, num_heads, H_ * window_size[0], W_ * window_size[1]\n",
    "    ).reshape(\n",
    "        batch_size, num_heads, H*W\n",
    "    )                                                                   # [B, num_heads, L]\n",
    "    delta_onehot = F.one_hot(delta_index, L_).float()                   # [B, num_heads, L, L']\n",
    "    relative_position_bias_init = getMask((H, W), delta_onehot)    # [B, num_heads, L, L']\n",
    "    return delta_onehot, relative_position_bias_init\n",
    "\n",
    "def initCenter(x: torch.Tensor, stride: int):\n",
    "    '''\n",
    "    输入: \n",
    "    x: tensor, [batch_size, num_heads, H, W, head_embed_dims], 待聚类数据\n",
    "    输出:\n",
    "    c_init: tensor, [batch_size, num_heads, L', head_embed_dims], 聚类中心\n",
    "    '''\n",
    "    batch_size, num_heads, H, W, head_embed_dims = x.shape\n",
    "\n",
    "    # [batch_size*num_heads, head_embed_dims, H, W]\n",
    "    x_temp = x.permute(\n",
    "        0, 1, 4, 2, 3\n",
    "    ).reshape(\n",
    "        batch_size*num_heads, head_embed_dims, H, W\n",
    "    )\n",
    "    # [batch_size, num_heads, L', head_embed_dims]\n",
    "    c_init = F.interpolate(x_temp, scale_factor=1/stride, mode='nearest').reshape(\n",
    "        batch_size, num_heads, head_embed_dims, -1\n",
    "    ).transpose(-2, -1)\n",
    "    # 单位化c_init, 便于后续进行余弦相似度计算\n",
    "    c_init = F.normalize(c_init, dim=-1)\n",
    "    return c_init\n",
    "\n",
    "def updateCenter(x: torch.Tensor, c: torch.Tensor, relative_position_bias: None):\n",
    "    '''\n",
    "    输入: \n",
    "    x: tensor, [batch_size, num_heads, H, W, head_embed_dims], 待聚类数据\n",
    "    c: tensor, [batch_size, num_heads, L', head_embed_dims], 初始化聚类中心\n",
    "    relative_position_bias: tensor, [batch_size, num_heads, L, L'], 相对位置编码\n",
    "    输出：\n",
    "    c_new: tensor, [B, num_heads, L', head_embed_dims], 更新后的聚类中心\n",
    "    affinity: \n",
    "    '''\n",
    "    batch_size, num_heads, H, W, head_embed_dims = x.shape\n",
    "    _, _, L_, _ = c.shape\n",
    "\n",
    "    x = x.reshape(batch_size, num_heads, -1, head_embed_dims)\n",
    "\n",
    "    # [B, num_heads, L, L']\n",
    "    scale = torch.tensor(15.0)\n",
    "    affinity_raw = torch.einsum('bhld,bhmd->bhlm', x, c)\n",
    "    affinity = affinity_raw  * scale\n",
    "    if not relative_position_bias is None:\n",
    "        affinity = affinity + relative_position_bias\n",
    "    \n",
    "    affinity_aux = affinity_raw * scale.detach()\n",
    "    if not relative_position_bias is None:\n",
    "        affinity_aux = affinity_aux + relative_position_bias\n",
    "\n",
    "    # 使用掩膜进行非极大值抑制\n",
    "    affinity_mask = torch.zeros_like(affinity)\n",
    "    affinity_mask[affinity < affinity.max(dim=-1, keepdim=True)[0]] = -torch.inf\n",
    "    # [B, num_heads, L, L']\n",
    "    affinity_onehot = torch.softmax(affinity + affinity_mask, dim=-1)\n",
    "\n",
    "    # 更新聚类中心\n",
    "    # [B, num_heads, L', head_embed_dims]\n",
    "    c_sum = torch.einsum('bhlm,bhld->bhmd', affinity_onehot, x)\n",
    "    # 直接单位化，就不用affinity归一化了\n",
    "    c_new = F.normalize(c_sum, dim=-1)\n",
    "\n",
    "    return c_new, affinity_onehot, affinity_aux, scale.detach()\n",
    "\n",
    "def getAffinity_label(labels: torch.Tensor, stride: int = 4, num_classes: int = 150):\n",
    "    '''\n",
    "    labels: [B, 1, H, W]\n",
    "    stride: 下采样倍率\n",
    "    num_classes: 分割类别数\n",
    "    '''\n",
    "    B, dim, H, W = labels.shape\n",
    "    assert dim == 1\n",
    "\n",
    "    # 分块，模拟下采样的过程，更加精细\n",
    "    bs = stride ** 2                                    # block size\n",
    "    nb = int(H*W / bs)                                  # num blocks\n",
    "    labels_unfold = F.unfold(                           # [B, nb, bs, dim]\n",
    "        labels.float(), \n",
    "        kernel_size=stride, \n",
    "        padding=0, \n",
    "        stride=stride\n",
    "    ).reshape(B, bs, nb).transpose(-2, -1).long()       # [B, dim*bs, nb] -> [B, nb, bs]\n",
    "\n",
    "    # 用于区分聚类中心每一行的编码\n",
    "    index_add = torch.arange(0, B*nb, device=labels.device).reshape(B, nb, 1) * num_classes\n",
    "    labels_unfold_encode = labels_unfold + index_add\n",
    "\n",
    "    # 统计聚类中心的类别成分占比\n",
    "    labels_unfold_encode_flatten = labels_unfold_encode.flatten()\n",
    "    counts = torch.bincount(labels_unfold_encode_flatten)\n",
    "    padding = B * nb * num_classes - counts.shape[0]    # 最后一行特殊处理\n",
    "    counts = torch.cat([counts, torch.zeros(padding, dtype=torch.long, device=labels.device)], dim=0)\n",
    "    frequency = counts.reshape(B, nb, num_classes) / bs # [B, nb, num_classes]\n",
    "\n",
    "    # 使用labels索引从聚类中心占比中索引出一个亲和力矩阵，亲和力矩阵的每个元素为labels元素在聚类中心分块中同类元素的占比\n",
    "    labels = labels.flatten(1).long()                   # [B, L]\n",
    "    affinity = []\n",
    "    for i in range(B):\n",
    "        affinity.append(frequency[i][:, labels[i]].transpose(-2, -1))\n",
    "    affinity = torch.stack(affinity).unsqueeze(1)       # [B, 1, L, nb]\n",
    "    return affinity\n",
    "\n",
    "# 1. 构造原始数据: [batch_size, num_heads, H, W, head_embed_dims]\n",
    "batch_size = 1\n",
    "num_heads = 1\n",
    "H, W = 4, 4\n",
    "stride = 2\n",
    "H_, W_ = H // stride, W // stride\n",
    "L_ = H_ * W_\n",
    "head_embed_dims = 4\n",
    "\n",
    "x_label = torch.randint(0, head_embed_dims, (batch_size, 1, H_, W_, 1))  # [batch_size, 1, H, W]\n",
    "x_label = x_label.repeat(\n",
    "    1, 1, 1, 1, stride**2\n",
    ").reshape(\n",
    "    batch_size, 1, H_, W_, stride, stride\n",
    ").permute(\n",
    "    0, 1, 2, 4, 3, 5\n",
    ").reshape(\n",
    "    batch_size, 1, H, W\n",
    ")\n",
    "# print('x_label: {}\\n{}'.format(x_label.shape, x_label))\n",
    "\n",
    "x_feature = F.one_hot(x_label.repeat(1, num_heads, 1, 1), head_embed_dims).float()\n",
    "# print('x_feature: {}\\n{}'.format(x_feature.shape, x_feature))\n",
    "\n",
    "# 2. 计算affinity_aux\n",
    "c = initCenter(x_feature, stride)   # [batch_size, num_heads, head_embed_dims, L_]\n",
    "# print(c)\n",
    "_, relative_position_bias = initRPE(x_feature.shape, c.shape, (stride, stride))\n",
    "# print('relative_position_bias: {}\\n{}'.format(relative_position_bias.shape, relative_position_bias))\n",
    "# _, _, affinity_aux, scale = updateCenter(x_feature, c, relative_position_bias=None)       # 无相对位置编码\n",
    "# print('affinity_aux_nopoe: {}\\n{}'.format(affinity_aux.shape, affinity_aux))\n",
    "_, _, affinity_aux, scale = updateCenter(x_feature, c, relative_position_bias)              # 有相对位置编码\n",
    "# print('affinity_aux: {}\\n{}'.format(affinity_aux.shape, affinity_aux))\n",
    "\n",
    "# 3. 构造affinity_label\n",
    "affinity_label = getAffinity_label(x_label, stride, head_embed_dims+1) * scale + relative_position_bias     # [batch_size, num_heads, L, L']\n",
    "# print('affinity_label: {}\\n{}'.format(affinity_label.shape, affinity_label))\n",
    "\n",
    "# 4. 计算affinity loss\n",
    "affinity_log_softmax = F.log_softmax(affinity_aux, dim=-1)  # [batch_size, num_heads, L, L']\n",
    "affinity_label_softmax = F.softmax(affinity_label, dim=-1)  # [batch_size, num_heads, L, L']\n",
    "\n",
    "# print(affinity_log_softmax.shape)\n",
    "# print(affinity_label_softmax.shape)\n",
    "\n",
    "KLDivLoss = nn.KLDivLoss(reduction='batchmean', log_target=False)\n",
    "loss = KLDivLoss(affinity_log_softmax.flatten(0,-2), affinity_label_softmax.flatten(0,-2))\n",
    "print('affinity loss: {:.6f}'.format(loss))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
