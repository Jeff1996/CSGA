{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4])\n",
      "torch.Size([1, 4, 4, 3])\n",
      "mask:\n",
      " tensor([[[[1., 1., 0., 1.],\n",
      "          [1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 0., 0., 0.]]]])\n",
      "mask_patches:\n",
      " tensor([[[1., 1., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 0., 0.]]])\n",
      "window_avg_pooling:\n",
      " tensor([[[15., 18., 21.],\n",
      "         [ 9., 10., 11.],\n",
      "         [87., 90., 93.],\n",
      "         [63., 65., 67.]]])\n",
      "img:\n",
      " tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.],\n",
      "          [ 9., 10., 11.]],\n",
      "\n",
      "         [[12., 13., 14.],\n",
      "          [15., 16., 17.],\n",
      "          [18., 19., 20.],\n",
      "          [21., 22., 23.]],\n",
      "\n",
      "         [[24., 25., 26.],\n",
      "          [27., 28., 29.],\n",
      "          [30., 31., 32.],\n",
      "          [33., 34., 35.]],\n",
      "\n",
      "         [[36., 37., 38.],\n",
      "          [39., 40., 41.],\n",
      "          [42., 43., 44.],\n",
      "          [45., 46., 47.]]]])\n",
      "img_patches:\n",
      " tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]],\n",
      "\n",
      "         [[ 6.,  7.,  8.],\n",
      "          [ 9., 10., 11.],\n",
      "          [18., 19., 20.],\n",
      "          [21., 22., 23.]],\n",
      "\n",
      "         [[24., 25., 26.],\n",
      "          [27., 28., 29.],\n",
      "          [36., 37., 38.],\n",
      "          [39., 40., 41.]],\n",
      "\n",
      "         [[30., 31., 32.],\n",
      "          [33., 34., 35.],\n",
      "          [42., 43., 44.],\n",
      "          [45., 46., 47.]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41227/2929050846.py:11: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  img = torch.range(0, B*C*H*W-1).reshape(B, H, W, C).float()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "B = 1\n",
    "C = 3\n",
    "H = 4\n",
    "W = 4\n",
    "\n",
    "patch_number = 2\n",
    "\n",
    "# mask = torch.randint(0, 2, (B, 1, H, W)).float()\n",
    "# img = torch.range(0, B*C*H*W-1).reshape(B, H, W, C).float()\n",
    "\n",
    "mask = torch.Tensor(\n",
    "    [                               # B\n",
    "        [                           # 1\n",
    "            [                       # H\n",
    "                [1., 1., 0., 1.],   # W\n",
    "                [1., 0., 0., 0.],\n",
    "                [1., 1., 1., 1.],\n",
    "                [1., 0., 0., 0.]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 掩码分块\n",
    "H_patch = H // patch_number\n",
    "W_patch = W // patch_number\n",
    "mask_patches = mask.reshape(\n",
    "    B, patch_number, H_patch, patch_number, W_patch\n",
    ").permute(\n",
    "    0, 1, 3, 2, 4\n",
    ").reshape(\n",
    "    B, -1, H_patch*W_patch\n",
    ")      \n",
    "\n",
    "# 图像分块\n",
    "img_patches = img.reshape(\n",
    "    B, patch_number, H_patch, patch_number, W_patch, C\n",
    ").permute(\n",
    "    0, 1, 3, 2, 4, 5\n",
    ").reshape(\n",
    "    B, -1, H_patch*W_patch, C\n",
    ")             \n",
    "\n",
    "# 分块均值池化\n",
    "window_avg_pooling = torch.einsum('bsl,bsld->bsd', mask_patches, img_patches)\n",
    "\n",
    "print('mask:\\n', mask)\n",
    "print('mask_patches:\\n', mask_patches)\n",
    "\n",
    "print('window_avg_pooling:\\n', window_avg_pooling)\n",
    "\n",
    "print('img:\\n', img)\n",
    "print('img_patches:\\n', img_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([inf])\n",
      "tensor([-inf])\n",
      "tensor([-inf])\n",
      "tensor([-0.])\n",
      "tensor([inf])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# inf的四则运算规则测试\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([-1.2], dtype=torch.float32)\n",
    "b = torch.tensor([torch.inf], dtype=torch.float32)\n",
    "\n",
    "print(a + b)\n",
    "print(a - b)\n",
    "print(a * b)\n",
    "print(a / b)\n",
    "print(torch.exp(b))\n",
    "print(torch.exp(-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 2, 3]])\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# cat 与 stack的深度复制测试\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3]).unsqueeze(0)\n",
    "b = torch.cat([a, a], dim=0)\n",
    "\n",
    "a[0][0] = 5\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]]), tensor([[1, 2],\n",
      "        [3, 4]])]\n"
     ]
    }
   ],
   "source": [
    "# 使用for循环构造列表的测试\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "\n",
    "print([\n",
    "    torch.stack((a, b), dim=0) for i in range(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 4\n",
      "1 2 5\n",
      "2 3 6\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(zip([1, 2, 3], [4, 5, 6])):\n",
    "    print(i, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 3])\n",
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Linear(4, 3, bias=True)\n",
    "data = torch.randn((16, 16, 4), dtype=torch.float32)\n",
    "\n",
    "out = net(data)\n",
    "print(out.shape)\n",
    "\n",
    "print((*data.shape[:2], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4]\n",
      "tensor([ 2,  4, 22, 24])\n"
     ]
    }
   ],
   "source": [
    "# DCAMA代码测试\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import torch\n",
    "\n",
    "feat_channels = [128, 256, 512, 1024]\n",
    "nlayers = [2, 2, 18, 2]\n",
    "\n",
    "lids = reduce(add, [[i + 1] * x for i, x in enumerate(nlayers)])\n",
    "stack_ids = torch.tensor(lids).bincount()[-4:].cumsum(dim=0)\n",
    "print(lids)\n",
    "print(stack_ids)\n",
    "'''\n",
    "DCAMA中，swin-b骨干网络的输出特征图情况如下（输入图片尺寸为[2, 3, 384, 384]）:\n",
    "torch.Size([2, 9216, 128])\n",
    "torch.Size([2, 9216, 128])\n",
    "\n",
    "torch.Size([2, 2304, 256])\n",
    "torch.Size([2, 2304, 256])\n",
    "\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "torch.Size([2, 576, 512])\n",
    "\n",
    "torch.Size([2, 144, 1024])\n",
    "torch.Size([2, 144, 1024])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:\n",
      " torch.return_types.max(\n",
      "values=tensor([[[[-0.6337,  0.7852, -0.6930,  0.2895],\n",
      "          [ 1.6348,  0.8421,  2.3349,  0.7589],\n",
      "          [ 1.3289,  1.3033, -0.0552, -0.0970],\n",
      "          [ 0.5299, -0.2014,  1.4027, -0.1976]],\n",
      "\n",
      "         [[ 1.2400, -0.8829, -0.3356,  2.1629],\n",
      "          [ 1.0375,  0.8535, -0.0611,  0.4860],\n",
      "          [ 1.6303,  0.5101, -0.7879,  0.5074],\n",
      "          [ 1.1449,  1.7214,  1.4532,  1.9156]],\n",
      "\n",
      "         [[ 0.2400,  1.3669,  1.1057,  1.1713],\n",
      "          [ 0.6118,  1.2392,  2.4902,  1.2335],\n",
      "          [-0.3423,  0.9491,  1.2006,  1.9659],\n",
      "          [ 0.3903,  0.9943,  0.1393,  1.6231]]]]),\n",
      "indices=tensor([[[[0, 1, 1, 1],\n",
      "          [0, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[0, 0, 1, 1],\n",
      "          [0, 1, 0, 0],\n",
      "          [1, 0, 1, 1],\n",
      "          [1, 1, 1, 0]],\n",
      "\n",
      "         [[1, 1, 0, 1],\n",
      "          [1, 1, 0, 1],\n",
      "          [1, 0, 1, 0],\n",
      "          [1, 0, 1, 0]]]]))\n",
      "c:\n",
      " tensor([[[[-0.6337,  0.7852, -0.6930,  0.2895],\n",
      "          [ 1.6348,  0.8421,  2.3349,  0.7589],\n",
      "          [ 1.3289,  1.3033, -0.0552, -0.0970],\n",
      "          [ 0.5299, -0.2014,  1.4027, -0.1976]],\n",
      "\n",
      "         [[ 1.2400, -0.8829, -0.3356,  2.1629],\n",
      "          [ 1.0375,  0.8535, -0.0611,  0.4860],\n",
      "          [ 1.6303,  0.5101, -0.7879,  0.5074],\n",
      "          [ 1.1449,  1.7214,  1.4532,  1.9156]],\n",
      "\n",
      "         [[ 0.2400,  1.3669,  1.1057,  1.1713],\n",
      "          [ 0.6118,  1.2392,  2.4902,  1.2335],\n",
      "          [-0.3423,  0.9491,  1.2006,  1.9659],\n",
      "          [ 0.3903,  0.9943,  0.1393,  1.6231]]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.max方法测试\n",
    "import torch\n",
    "\n",
    "a = torch.randn((2, 3, 4, 4), dtype=torch.float32)\n",
    "b = torch.max(a, dim=0, keepdim=True)\n",
    "c = b.values\n",
    "print('b:\\n', b)\n",
    "\n",
    "print('c:\\n', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[3, 8],\n",
      "          [1, 4]]],\n",
      "\n",
      "\n",
      "        [[[8, 9],\n",
      "          [3, 5]]],\n",
      "\n",
      "\n",
      "        [[[5, 2],\n",
      "          [0, 7]]]])\n",
      "tensor([[[[8, 9],\n",
      "          [3, 5]]],\n",
      "\n",
      "\n",
      "        [[[5, 2],\n",
      "          [0, 7]]],\n",
      "\n",
      "\n",
      "        [[[3, 8],\n",
      "          [1, 4]]]])\n",
      "[2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# tensor切片测试\n",
    "import torch\n",
    "\n",
    "a = torch.randint(0, 10, (3, 1, 2, 2))\n",
    "print(a)\n",
    "\n",
    "print(torch.cat((a[1:],a[:1]), dim=0))\n",
    "\n",
    "\n",
    "b = [1, 2, 3]\n",
    "print(b[1:] + b[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.backbone.patch_embed.proj.weight                               , torch.Size([128, 3, 4, 4])\n",
      "module.backbone.patch_embed.proj.bias                                 , torch.Size([128])\n",
      "module.backbone.patch_embed.norm.weight                               , torch.Size([128])\n",
      "module.backbone.patch_embed.norm.bias                                 , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.0.norm1.weight                        , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.0.norm1.bias                          , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.0.attn.relative_position_bias_table   , torch.Size([529, 4])\n",
      "module.backbone.layers.0.blocks.0.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.0.blocks.0.attn.qkv.weight                     , torch.Size([384, 128])\n",
      "module.backbone.layers.0.blocks.0.attn.qkv.bias                       , torch.Size([384])\n",
      "module.backbone.layers.0.blocks.0.attn.proj.weight                    , torch.Size([128, 128])\n",
      "module.backbone.layers.0.blocks.0.attn.proj.bias                      , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.0.norm2.weight                        , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.0.norm2.bias                          , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.0.mlp.fc1.weight                      , torch.Size([512, 128])\n",
      "module.backbone.layers.0.blocks.0.mlp.fc1.bias                        , torch.Size([512])\n",
      "module.backbone.layers.0.blocks.0.mlp.fc2.weight                      , torch.Size([128, 512])\n",
      "module.backbone.layers.0.blocks.0.mlp.fc2.bias                        , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.1.attn_mask                           , torch.Size([64, 144, 144])\n",
      "module.backbone.layers.0.blocks.1.norm1.weight                        , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.1.norm1.bias                          , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.1.attn.relative_position_bias_table   , torch.Size([529, 4])\n",
      "module.backbone.layers.0.blocks.1.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.0.blocks.1.attn.qkv.weight                     , torch.Size([384, 128])\n",
      "module.backbone.layers.0.blocks.1.attn.qkv.bias                       , torch.Size([384])\n",
      "module.backbone.layers.0.blocks.1.attn.proj.weight                    , torch.Size([128, 128])\n",
      "module.backbone.layers.0.blocks.1.attn.proj.bias                      , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.1.norm2.weight                        , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.1.norm2.bias                          , torch.Size([128])\n",
      "module.backbone.layers.0.blocks.1.mlp.fc1.weight                      , torch.Size([512, 128])\n",
      "module.backbone.layers.0.blocks.1.mlp.fc1.bias                        , torch.Size([512])\n",
      "module.backbone.layers.0.blocks.1.mlp.fc2.weight                      , torch.Size([128, 512])\n",
      "module.backbone.layers.0.blocks.1.mlp.fc2.bias                        , torch.Size([128])\n",
      "module.backbone.layers.0.downsample.reduction.weight                  , torch.Size([256, 512])\n",
      "module.backbone.layers.0.downsample.norm.weight                       , torch.Size([512])\n",
      "module.backbone.layers.0.downsample.norm.bias                         , torch.Size([512])\n",
      "module.backbone.layers.1.blocks.0.norm1.weight                        , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.0.norm1.bias                          , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.0.attn.relative_position_bias_table   , torch.Size([529, 8])\n",
      "module.backbone.layers.1.blocks.0.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.1.blocks.0.attn.qkv.weight                     , torch.Size([768, 256])\n",
      "module.backbone.layers.1.blocks.0.attn.qkv.bias                       , torch.Size([768])\n",
      "module.backbone.layers.1.blocks.0.attn.proj.weight                    , torch.Size([256, 256])\n",
      "module.backbone.layers.1.blocks.0.attn.proj.bias                      , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.0.norm2.weight                        , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.0.norm2.bias                          , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.0.mlp.fc1.weight                      , torch.Size([1024, 256])\n",
      "module.backbone.layers.1.blocks.0.mlp.fc1.bias                        , torch.Size([1024])\n",
      "module.backbone.layers.1.blocks.0.mlp.fc2.weight                      , torch.Size([256, 1024])\n",
      "module.backbone.layers.1.blocks.0.mlp.fc2.bias                        , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.1.attn_mask                           , torch.Size([16, 144, 144])\n",
      "module.backbone.layers.1.blocks.1.norm1.weight                        , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.1.norm1.bias                          , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.1.attn.relative_position_bias_table   , torch.Size([529, 8])\n",
      "module.backbone.layers.1.blocks.1.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.1.blocks.1.attn.qkv.weight                     , torch.Size([768, 256])\n",
      "module.backbone.layers.1.blocks.1.attn.qkv.bias                       , torch.Size([768])\n",
      "module.backbone.layers.1.blocks.1.attn.proj.weight                    , torch.Size([256, 256])\n",
      "module.backbone.layers.1.blocks.1.attn.proj.bias                      , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.1.norm2.weight                        , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.1.norm2.bias                          , torch.Size([256])\n",
      "module.backbone.layers.1.blocks.1.mlp.fc1.weight                      , torch.Size([1024, 256])\n",
      "module.backbone.layers.1.blocks.1.mlp.fc1.bias                        , torch.Size([1024])\n",
      "module.backbone.layers.1.blocks.1.mlp.fc2.weight                      , torch.Size([256, 1024])\n",
      "module.backbone.layers.1.blocks.1.mlp.fc2.bias                        , torch.Size([256])\n",
      "module.backbone.layers.1.downsample.reduction.weight                  , torch.Size([512, 1024])\n",
      "module.backbone.layers.1.downsample.norm.weight                       , torch.Size([1024])\n",
      "module.backbone.layers.1.downsample.norm.bias                         , torch.Size([1024])\n",
      "module.backbone.layers.2.blocks.0.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.0.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.0.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.0.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.0.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.0.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.0.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.0.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.0.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.0.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.0.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.0.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.0.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.0.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.1.attn_mask                           , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.1.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.1.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.1.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.1.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.1.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.1.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.1.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.1.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.1.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.1.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.1.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.1.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.1.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.1.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.2.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.2.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.2.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.2.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.2.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.2.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.2.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.2.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.2.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.2.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.2.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.2.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.2.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.2.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.3.attn_mask                           , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.3.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.3.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.3.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.3.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.3.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.3.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.3.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.3.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.3.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.3.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.3.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.3.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.3.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.3.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.4.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.4.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.4.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.4.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.4.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.4.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.4.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.4.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.4.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.4.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.4.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.4.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.4.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.4.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.5.attn_mask                           , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.5.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.5.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.5.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.5.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.5.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.5.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.5.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.5.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.5.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.5.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.5.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.5.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.5.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.5.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.6.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.6.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.6.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.6.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.6.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.6.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.6.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.6.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.6.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.6.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.6.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.6.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.6.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.6.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.7.attn_mask                           , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.7.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.7.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.7.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.7.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.7.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.7.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.7.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.7.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.7.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.7.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.7.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.7.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.7.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.7.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.8.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.8.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.8.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.8.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.8.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.8.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.8.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.8.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.8.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.8.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.8.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.8.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.8.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.8.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.9.attn_mask                           , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.9.norm1.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.9.norm1.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.9.attn.relative_position_bias_table   , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.9.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.9.attn.qkv.weight                     , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.9.attn.qkv.bias                       , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.9.attn.proj.weight                    , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.9.attn.proj.bias                      , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.9.norm2.weight                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.9.norm2.bias                          , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.9.mlp.fc1.weight                      , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.9.mlp.fc1.bias                        , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.9.mlp.fc2.weight                      , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.9.mlp.fc2.bias                        , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.10.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.10.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.10.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.10.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.10.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.10.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.10.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.10.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.10.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.10.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.10.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.10.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.10.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.10.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.11.attn_mask                          , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.11.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.11.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.11.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.11.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.11.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.11.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.11.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.11.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.11.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.11.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.11.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.11.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.11.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.11.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.12.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.12.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.12.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.12.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.12.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.12.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.12.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.12.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.12.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.12.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.12.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.12.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.12.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.12.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.13.attn_mask                          , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.13.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.13.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.13.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.13.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.13.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.13.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.13.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.13.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.13.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.13.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.13.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.13.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.13.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.13.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.14.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.14.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.14.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.14.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.14.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.14.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.14.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.14.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.14.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.14.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.14.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.14.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.14.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.14.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.15.attn_mask                          , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.15.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.15.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.15.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.15.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.15.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.15.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.15.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.15.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.15.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.15.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.15.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.15.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.15.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.15.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.16.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.16.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.16.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.16.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.16.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.16.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.16.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.16.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.16.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.16.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.16.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.16.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.16.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.16.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.17.attn_mask                          , torch.Size([4, 144, 144])\n",
      "module.backbone.layers.2.blocks.17.norm1.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.17.norm1.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.17.attn.relative_position_bias_table  , torch.Size([529, 16])\n",
      "module.backbone.layers.2.blocks.17.attn.relative_position_index       , torch.Size([144, 144])\n",
      "module.backbone.layers.2.blocks.17.attn.qkv.weight                    , torch.Size([1536, 512])\n",
      "module.backbone.layers.2.blocks.17.attn.qkv.bias                      , torch.Size([1536])\n",
      "module.backbone.layers.2.blocks.17.attn.proj.weight                   , torch.Size([512, 512])\n",
      "module.backbone.layers.2.blocks.17.attn.proj.bias                     , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.17.norm2.weight                       , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.17.norm2.bias                         , torch.Size([512])\n",
      "module.backbone.layers.2.blocks.17.mlp.fc1.weight                     , torch.Size([2048, 512])\n",
      "module.backbone.layers.2.blocks.17.mlp.fc1.bias                       , torch.Size([2048])\n",
      "module.backbone.layers.2.blocks.17.mlp.fc2.weight                     , torch.Size([512, 2048])\n",
      "module.backbone.layers.2.blocks.17.mlp.fc2.bias                       , torch.Size([512])\n",
      "module.backbone.layers.2.downsample.reduction.weight                  , torch.Size([1024, 2048])\n",
      "module.backbone.layers.2.downsample.norm.weight                       , torch.Size([2048])\n",
      "module.backbone.layers.2.downsample.norm.bias                         , torch.Size([2048])\n",
      "module.backbone.layers.3.blocks.0.norm1.weight                        , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.0.norm1.bias                          , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.0.attn.relative_position_bias_table   , torch.Size([529, 32])\n",
      "module.backbone.layers.3.blocks.0.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.3.blocks.0.attn.qkv.weight                     , torch.Size([3072, 1024])\n",
      "module.backbone.layers.3.blocks.0.attn.qkv.bias                       , torch.Size([3072])\n",
      "module.backbone.layers.3.blocks.0.attn.proj.weight                    , torch.Size([1024, 1024])\n",
      "module.backbone.layers.3.blocks.0.attn.proj.bias                      , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.0.norm2.weight                        , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.0.norm2.bias                          , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.0.mlp.fc1.weight                      , torch.Size([4096, 1024])\n",
      "module.backbone.layers.3.blocks.0.mlp.fc1.bias                        , torch.Size([4096])\n",
      "module.backbone.layers.3.blocks.0.mlp.fc2.weight                      , torch.Size([1024, 4096])\n",
      "module.backbone.layers.3.blocks.0.mlp.fc2.bias                        , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.1.norm1.weight                        , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.1.norm1.bias                          , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.1.attn.relative_position_bias_table   , torch.Size([529, 32])\n",
      "module.backbone.layers.3.blocks.1.attn.relative_position_index        , torch.Size([144, 144])\n",
      "module.backbone.layers.3.blocks.1.attn.qkv.weight                     , torch.Size([3072, 1024])\n",
      "module.backbone.layers.3.blocks.1.attn.qkv.bias                       , torch.Size([3072])\n",
      "module.backbone.layers.3.blocks.1.attn.proj.weight                    , torch.Size([1024, 1024])\n",
      "module.backbone.layers.3.blocks.1.attn.proj.bias                      , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.1.norm2.weight                        , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.1.norm2.bias                          , torch.Size([1024])\n",
      "module.backbone.layers.3.blocks.1.mlp.fc1.weight                      , torch.Size([4096, 1024])\n",
      "module.backbone.layers.3.blocks.1.mlp.fc1.bias                        , torch.Size([4096])\n",
      "module.backbone.layers.3.blocks.1.mlp.fc2.weight                      , torch.Size([1024, 4096])\n",
      "module.backbone.layers.3.blocks.1.mlp.fc2.bias                        , torch.Size([1024])\n",
      "module.backbone.norm.weight                                           , torch.Size([1024])\n",
      "module.backbone.norm.bias                                             , torch.Size([1024])\n",
      "module.backbone.head.weight                                           , torch.Size([1000, 1024])\n",
      "module.backbone.head.bias                                             , torch.Size([1000])\n",
      "module.trans_learner.transformer.0.linears.0.weight                   , torch.Size([256, 256])\n",
      "module.trans_learner.transformer.0.linears.0.bias                     , torch.Size([256])\n",
      "module.trans_learner.transformer.0.linears.1.weight                   , torch.Size([256, 256])\n",
      "module.trans_learner.transformer.0.linears.1.bias                     , torch.Size([256])\n",
      "module.trans_learner.transformer.1.linears.0.weight                   , torch.Size([512, 512])\n",
      "module.trans_learner.transformer.1.linears.0.bias                     , torch.Size([512])\n",
      "module.trans_learner.transformer.1.linears.1.weight                   , torch.Size([512, 512])\n",
      "module.trans_learner.transformer.1.linears.1.bias                     , torch.Size([512])\n",
      "module.trans_learner.transformer.2.linears.0.weight                   , torch.Size([1024, 1024])\n",
      "module.trans_learner.transformer.2.linears.0.bias                     , torch.Size([1024])\n",
      "module.trans_learner.transformer.2.linears.1.weight                   , torch.Size([1024, 1024])\n",
      "module.trans_learner.transformer.2.linears.1.bias                     , torch.Size([1024])\n",
      "module.trans_learner.pe.0.pe                                          , torch.Size([1, 10000, 256])\n",
      "module.trans_learner.pe.1.pe                                          , torch.Size([1, 10000, 512])\n",
      "module.trans_learner.pe.2.pe                                          , torch.Size([1, 10000, 1024])\n",
      "module.trans_learner.encoder4.0.weight                                , torch.Size([16, 2, 3, 3])\n",
      "module.trans_learner.encoder4.0.bias                                  , torch.Size([16])\n",
      "module.trans_learner.encoder4.1.weight                                , torch.Size([16])\n",
      "module.trans_learner.encoder4.1.bias                                  , torch.Size([16])\n",
      "module.trans_learner.encoder4.3.weight                                , torch.Size([64, 16, 3, 3])\n",
      "module.trans_learner.encoder4.3.bias                                  , torch.Size([64])\n",
      "module.trans_learner.encoder4.4.weight                                , torch.Size([64])\n",
      "module.trans_learner.encoder4.4.bias                                  , torch.Size([64])\n",
      "module.trans_learner.encoder4.6.weight                                , torch.Size([128, 64, 3, 3])\n",
      "module.trans_learner.encoder4.6.bias                                  , torch.Size([128])\n",
      "module.trans_learner.encoder4.7.weight                                , torch.Size([128])\n",
      "module.trans_learner.encoder4.7.bias                                  , torch.Size([128])\n",
      "module.trans_learner.encoder3.0.weight                                , torch.Size([16, 18, 5, 5])\n",
      "module.trans_learner.encoder3.0.bias                                  , torch.Size([16])\n",
      "module.trans_learner.encoder3.1.weight                                , torch.Size([16])\n",
      "module.trans_learner.encoder3.1.bias                                  , torch.Size([16])\n",
      "module.trans_learner.encoder3.3.weight                                , torch.Size([64, 16, 3, 3])\n",
      "module.trans_learner.encoder3.3.bias                                  , torch.Size([64])\n",
      "module.trans_learner.encoder3.4.weight                                , torch.Size([64])\n",
      "module.trans_learner.encoder3.4.bias                                  , torch.Size([64])\n",
      "module.trans_learner.encoder3.6.weight                                , torch.Size([128, 64, 3, 3])\n",
      "module.trans_learner.encoder3.6.bias                                  , torch.Size([128])\n",
      "module.trans_learner.encoder3.7.weight                                , torch.Size([128])\n",
      "module.trans_learner.encoder3.7.bias                                  , torch.Size([128])\n",
      "module.trans_learner.encoder2.0.weight                                , torch.Size([16, 2, 5, 5])\n",
      "module.trans_learner.encoder2.0.bias                                  , torch.Size([16])\n",
      "module.trans_learner.encoder2.1.weight                                , torch.Size([16])\n",
      "module.trans_learner.encoder2.1.bias                                  , torch.Size([16])\n",
      "module.trans_learner.encoder2.3.weight                                , torch.Size([64, 16, 5, 5])\n",
      "module.trans_learner.encoder2.3.bias                                  , torch.Size([64])\n",
      "module.trans_learner.encoder2.4.weight                                , torch.Size([64])\n",
      "module.trans_learner.encoder2.4.bias                                  , torch.Size([64])\n",
      "module.trans_learner.encoder2.6.weight                                , torch.Size([128, 64, 3, 3])\n",
      "module.trans_learner.encoder2.6.bias                                  , torch.Size([128])\n",
      "module.trans_learner.encoder2.7.weight                                , torch.Size([128])\n",
      "module.trans_learner.encoder2.7.bias                                  , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.0.weight                       , torch.Size([128, 128, 3, 3])\n",
      "module.trans_learner.encoder_layer4to3.0.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.1.weight                       , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.1.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.3.weight                       , torch.Size([128, 128, 3, 3])\n",
      "module.trans_learner.encoder_layer4to3.3.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.4.weight                       , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.4.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.6.weight                       , torch.Size([128, 128, 3, 3])\n",
      "module.trans_learner.encoder_layer4to3.6.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.7.weight                       , torch.Size([128])\n",
      "module.trans_learner.encoder_layer4to3.7.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.0.weight                       , torch.Size([128, 128, 3, 3])\n",
      "module.trans_learner.encoder_layer3to2.0.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.1.weight                       , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.1.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.3.weight                       , torch.Size([128, 128, 3, 3])\n",
      "module.trans_learner.encoder_layer3to2.3.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.4.weight                       , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.4.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.6.weight                       , torch.Size([128, 128, 3, 3])\n",
      "module.trans_learner.encoder_layer3to2.6.bias                         , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.7.weight                       , torch.Size([128])\n",
      "module.trans_learner.encoder_layer3to2.7.bias                         , torch.Size([128])\n",
      "module.trans_learner.decoder1.0.weight                                , torch.Size([128, 896, 3, 3])\n",
      "module.trans_learner.decoder1.0.bias                                  , torch.Size([128])\n",
      "module.trans_learner.decoder1.2.weight                                , torch.Size([64, 128, 3, 3])\n",
      "module.trans_learner.decoder1.2.bias                                  , torch.Size([64])\n",
      "module.trans_learner.decoder2.0.weight                                , torch.Size([64, 64, 3, 3])\n",
      "module.trans_learner.decoder2.0.bias                                  , torch.Size([64])\n",
      "module.trans_learner.decoder2.2.weight                                , torch.Size([16, 64, 3, 3])\n",
      "module.trans_learner.decoder2.2.bias                                  , torch.Size([16])\n",
      "module.trans_learner.decoder3.0.weight                                , torch.Size([16, 16, 3, 3])\n",
      "module.trans_learner.decoder3.0.bias                                  , torch.Size([16])\n",
      "module.trans_learner.decoder3.2.weight                                , torch.Size([2, 16, 3, 3])\n",
      "module.trans_learner.decoder3.2.bias                                  , torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# DCAMA权重文件转换\n",
    "\n",
    "import torch\n",
    "\n",
    "checkpoint_file = '/home/hjf/workspace/mmsegmentation/work_dirs/dcama_pretrained/pascal-5i/swin_fold0.pt'\n",
    "# checkpoint_file = '/home/hjf/workspace/mmsegmentation/work_dirs/dcama_pretrained/coco-20i/swin_fold0.pt'\n",
    "\n",
    "params = torch.load(checkpoint_file, map_location='cpu')\n",
    "for key in params:\n",
    "    # if 'backbone' in key:\n",
    "    #     continue\n",
    "    print('{:70s}, {}'.format(key, params[key].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_fg\n",
      " tensor([[[1, 0, 0, 1],\n",
      "         [1, 1, 0, 0],\n",
      "         [0, 0, 1, 1],\n",
      "         [1, 0, 0, 0]]])\n",
      "label_bg\n",
      " tensor([[[0, 1, 1, 0],\n",
      "         [0, 0, 1, 1],\n",
      "         [1, 1, 0, 0],\n",
      "         [0, 1, 1, 1]]])\n",
      "label_fg_sum\n",
      " tensor([[2, 2, 2, 1]])\n",
      "label_bg_sum\n",
      " tensor([[2, 2, 2, 3]])\n",
      "label_fg_sum\n",
      " tensor([[2, 2, 2, 0]])\n",
      "label_bg_sum\n",
      " tensor([[0, 0, 0, 3]])\n",
      "label_fg\n",
      " tensor([[[1, 0, 0, 1],\n",
      "         [1, 1, 0, 0],\n",
      "         [0, 0, 1, 1],\n",
      "         [0, 0, 0, 0]]])\n",
      "label_bg\n",
      " tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "# tensor索引测试\n",
    "\n",
    "import torch\n",
    "\n",
    "label = torch.tensor([\n",
    "    [1, 0, 1, 1],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [1, 1, 0, 0]\n",
    "]).reshape(\n",
    "    1, 2, 2, 2, 2\n",
    ").permute(\n",
    "    0, 1, 3, 2, 4\n",
    ").reshape(1, 4, 4)\n",
    "\n",
    "label_fg = label\n",
    "label_bg = 1 - label\n",
    "print('label_fg\\n', label_fg)\n",
    "print('label_bg\\n', label_bg)\n",
    "\n",
    "label_fg_sum = label_fg.sum(dim=-1)\n",
    "label_bg_sum = label_bg.sum(dim=-1)\n",
    "print('label_fg_sum\\n', label_fg_sum)\n",
    "print('label_bg_sum\\n', label_bg_sum)\n",
    "\n",
    "label_fg_sum[label_fg_sum<label_bg_sum] = 1e-6\n",
    "label_bg_sum[label_fg_sum>=label_bg_sum] = 1e-6\n",
    "print('label_fg_sum\\n', label_fg_sum)\n",
    "print('label_bg_sum\\n', label_bg_sum)\n",
    "\n",
    "label_fg[label_fg_sum<1] *= 0\n",
    "label_bg[label_bg_sum<1] *= 0\n",
    "print('label_fg\\n', label_fg)\n",
    "print('label_bg\\n', label_bg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hjf_mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
